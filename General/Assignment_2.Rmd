---
title: "Assignment_2"
author: "Mona Behrouzian"
date: "10/10/2023"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#code from TA to make sure my commented code does not fall off the PDF page
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80), tidy=TRUE)
```


```{r}
setwd("C:\\Users\\monab\\Documents\\R")

ovarian.dataset <- read.delim("ovarian.data", sep=",", header = FALSE)
features <- c("perimeter", "area", "smoothness", "symmetry", "concavity", paste("protein", seq(1, 25), sep=""))
names(ovarian.dataset) <- c("cell_id", "diagnosis", features) 
# paste0(features,"_mean"), paste0(features,"_se"), paste0(features,"_worst"))

```

```{r}
#getting a sense of the data that has been loaded
#columns are: ID (not a feature), diagnosis plus the 5 variables & 25 proteins (ie 30 features)

#head(ovarian.dataset)
```

# Q1. DIMENSIONALITY REDUCTION
## Q1.1. Perform PCA on the features of the dataset. How much of the variation in the data is associated with PC1?
### 42.8% of the variation in the data is associated with PC1 
```{r}
# PCA code from tutorial 1. columns 3 to 32 only are features
# scales and centers the data, required for PCA
# Outputs the features as rows, and principle components as columns, or the axes 
ovarian.pca <- prcomp(ovarian.dataset[,c(3:32)], center = TRUE, scale. = TRUE)

# Gives the Important of components
#	Shows us the standard deviation and proportion of variance
summary(ovarian.pca)
```

## Q1.2. You want to represent 90% of the variance in the data by dimensionality reduction. How many PCs do you need to achieve this? In other words, what would be the dimensionality of the reduced feature space so that you preserve 90% of the variability in the data?
### Need 9 PCs for 90% of the variance
```{r}
# we are looking for a cumulative proportion of variance of 90%

# extract the standard deviation of the PCA
std.pca <- ovarian.pca$sdev

# variance is square of std
var.pca <- std.pca^2

# proportion of var is the variance divided by sum of all variances
prop.pca <- var.pca/sum(var.pca)

# cumulative proportion is the cumulative sum of proportion of variance 
c.sum.pca <- cumsum(prop.pca)

# use "which" function to find 1st instance when the cumsum is >=90% 
which(c.sum.pca >= 0.9)[1]

```
## Q1.3. In a 2-D plot, can you plot the observations corresponding to the first two important PCs? Note, use two different colors to represent the two classes of cells.

```{r}
#install.packages("devtools")
library(devtools)
#install.packages("remotes")
remotes::install_github("vqv/ggbiplot")
library(ggbiplot)
library(ggplot2)
library(plyr)
library(scales)
library(grid)

ggbiplot(ovarian.pca, choices = c(1,2), groups = ovarian.dataset$diagnosis)
```

## Q1.4. Can you plot the "area" and "concavity" features associated with the cells?
```{r}
#ASK TA : confirm it is okay that we used ggplot? does it look okay? -> yes its okay and yes it looks good 

library(ggplot2)
Diagnosis <- ovarian.dataset$diagnosis
ggplot(ovarian.dataset)+
  geom_point(aes(x=Area, y=Concavity, color = Diagnosis))

```

## Q1.5. What is the difference between the two plots? Which one gives you better separation between the classes and why?
```{r}
# the first plot gives better separation.

# LATERRRRRRRRRRRRRRRRR

```

## BONUS: Q1.6 (later! i think the TA said it is just 1 line of code)

```{r}
#LATERRRRRRRRRRRR
```


# Q2. CLUSTERING
*When comparing model predictions to true labels, obtain a confusion matrix and include this result in your submission. You can obtain this by using the table() function like so: table(predictions, labels)*

## Q2.1. Apply kmeans clustering on the data and identify two clusters within your dataset. What is the concordance between the clusters that you have identified and the true labels of the cells (Benign vs Malignant).

```{r}
#using code from the webpage linked in tutorial 2 by datanovia and factoextra

#install.packages("factoextra") # for beautiful graph of clusters
library(factoextra) #load library

#must scale data first
ovarian.features <- scale(ovarian.dataset[,c(3:32)]) 

# Compute k-means with k = 2
#set.seed(123)
kmeans2 <- kmeans(ovarian.features, 2, nstart = 25)

#plotting results
fviz_cluster(kmeans2, data = ovarian.features)
             

#print(km.res) # Print the results
```
```{r}
# What is the concordance between the clusters that you have identified and the true labels of the cells (Benign vs Malignant)?

#convert the clusters into diagnosis labels 
cluster <- kmeans2$cluster
predicted.diagnosis <- factor(ifelse(cluster == 1, "M", "B")) #code from Hint
expected.diagnosis <- factor(ovarian.dataset$diagnosis)

#Install required packages
#install.packages('caret')
library(caret) #library needed to create confusion matrix

#create confusion matrix
confusion.matrix <- confusionMatrix(predicted.diagnosis, expected.diagnosis)
confusion.matrix

```

## Q2.2. Repeat the kmeans analysis 10 times and report the mean accuracy across the 10 runs. Why are the results different in each run?

```{r}
### ASK TA!!! why is it giving 92% or 100-92% each time??? is that the whole point? -> fix code to subtract from 100%

ten.runs <- c(1:10)

for(i in 1:10){
  kmeans2 <- kmeans(ovarian.features, 2, nstart = 25)
  cluster <- kmeans2$cluster
  predicted.diagnosis <- factor(ifelse(cluster == 1, "B", "M")) #code from Hint
  # try mean(** == ***)
  confusion.matrix <- confusionMatrix(predicted.diagnosis, expected.diagnosis)
  accuracy <- confusion.matrix$overall['Accuracy']
  ten.runs[i] <- accuracy
  ### fix 
}
ten.runs*100

mean(ten.runs)*100
```

## Q2.3. Repeat the same analysis but with the top 5 PCs.

```{r}
#option 2 

ten.runs.pca <- c(1:10)

for(i in 1:10){
  data.kmean.pca <- kmeans(ovarian.pca$x[,1:5], 2, nstart = 25)
  cluster <- data.kmean.pca$cluster
  predicted.diagnosis <- factor(ifelse(cluster == 1, "B", "M")) #code from Hint\
  # try the thing
  confusion.matrix <- confusionMatrix(predicted.diagnosis, expected.diagnosis)
  accuracy <- confusion.matrix$overall['Accuracy']
  ten.runs.pca[i] <- accuracy
  ## fix 
}
ten.runs.pca*100

mean(ten.runs.pca)*100

```
## Q2.4. Compare the results between Q2.2. and Q2.3.
# The results get worse! Why? Perhaps it was overfitted 


### Q3. CLASSIFICATION
```{r}
#Divide your data into training and test sets using the following command:
ovarian.dataset.train <- ovarian.dataset[sample(nrow(ovarian.dataset))[1:(nrow(ovarian.dataset)/2)],]
ovarian.dataset.test <- ovarian.dataset[sample(nrow(ovarian.dataset))[(nrow(ovarian.dataset)/2):(nrow(ovarian.dataset))],]

# install.packages('ISLR') #in console
require(ISLR)
```
"sample" randomly takes half of the data as training data, other half for testing 


Need to see how our output is dependent on our input. To do that, we can use correlation, ie how much the
variation in our output is realted to our input. Or, if i change the input, how much that affects the output.
```{r}
# install this for correlation install.packages('corrplot')
library(corrplot)

correlations <- cor(ovarian.dataset.train[, 3:32])
#correlations

corrplot(correlations, method = "circle")

```
Now, we can see the correlation plot above. We used circles. Rows and cols are features. We are measuring correlation between alllll the pairs of features. Larger the circle, higher the correlation. Dark blue means huge correlation with the data

### Q3.1. Design a logistic regression classifier to identify (differentiate) benign and malignant cells. 
Report the performance of the classification technique on the training and test sets. 
You can report accuracy, precision and recall. 
Compare the performance of the classifier on the training and test set and provide a reason as to why one is better than the other.

??????? like the lecture suggestion do we need to review our training set?
if so, should we just look at the distribution of the data using box and whisker plots and use correlation plots?? like the tutorial showed? 
we didnt go deep into analyzing the results and how it affects what algorithm we use on the data 

3.1 use logistic regression, like the tutorial 
show the table!!! must! graded!
based on the table, can calculate several things : a and p and r
but dont do it manually. use code.

```{r}
#We need to first convert all the char variables to be numbers (help from TA!)
#note we don't know if M becomes 1 or 0, vice versa for B. We just trust the function.
ovarian.dataset.train$diagnosis <- as.factor(ovarian.dataset.train$diagnosis)

diagnosis.train <- ovarian.dataset.train$diagnosis
```


```{r}
#Creating the logistic regression model.
#using the binomial argument to make it clear that the intent is to git a logistic regression model
# the "." means we are taking all the features, but not including cell_id 
glm.train <- glm(diagnosis.train ~. - cell_id - diagnosis, data = ovarian.dataset.train, family = binomial)
summary(glm.train)

#note that this gives the error:
# "glm.fit: algorithm did not converge glm.fit: fitted probabilities numerically 0 or 1 occurred"
#but, that just means that the function was able to perfectly separate the variable into 0's and 1's.
#this is what we wanted!
#also the assignment says dont worry if the model does not converge 
```
Summary similar to what we saw for linear reg., eg p values etc. 
Note that ALLLLL the p-values are so HIGH!
Thus, none of the coefficients are significant here. 
To improve the model, you would drop the features based on which ones have high/bad p value 


We now have the model trained, but need to get the prediction from it.
Need to extract the probabilities from the model. Using the prediction function!
If you dont use the "type", it will NOT GIVE YOU THE PROBABILITY!!!!
```{r}
# input our model, then features, then the type. If you don't use the type, it will NOT GIVE YOU THE PROBABILITY!!!!
glm.probs.train <- predict(glm.train, ovarian.dataset.train, type = "response")
#i got an error when i only used col 3:32 of the dataset, asking that "cell_id" was not found... 

glm.probs.train[1:5]
```
These probabilities are very far from 0.5, ie they are extremely close to 0 or 1, which is good! Our model is classifying strongly. 

So we have the probabilities, but how to convert them to labels? 
Consider the typical s-shape logistic reg curve. If the prob is >0.5 it is yes, <0.5 it is 0. Because it is binary. 
This threshold is dependent on the application.
This will come up on the assignment as ROC.

We simply need to use ifelse to do this. It will convert each of the probabilities to the B or M label.
```{r}
glm.pred.train <- ifelse(glm.probs.train > 0.5, "M", "B")

#attach(ovarian.dataset.train)

prediction <- glm.pred.train
expected <- ovarian.dataset.train$diagnosis

table(prediction, expected)
```
??? concerned because there are NO false positives or negatives??? 
Does not make sense because the p-values we got were high, which is supposed to be bad for prediction 
. 

Using this table, we can manually calculate many statistics such as accuracy, precision, and recall of the prediction.
Here we get the avg. Comparing the predictions with the average data.
```{r}
confusionMatrix <- table(prediction, expected)

tb <- confusionMatrix[1,1] #true benign
tm <- confusionMatrix[2,2] #true malignant 
fb <- confusionMatrix[1,2] #false benign
fm <- confusionMatrix[2,1] #false malignant 

acc <- (tb+tm)/(tb+fb+fm+tm)
sprintf("Accuracy is: %f", acc)

prec <- tb/(tb+fb)
sprintf("Precision is: %f", prec)

recall <- tb/(tb+fm)
sprintf("Recall is: %f", recall)

#mean(glm.pred.train == expected)
#this code works in this case because it is a binary. 
```
??? Thus the accuracy of our model is 100% THATS NOT POSSIBLE

How could we improve the model? Drop the features with high p-value.


Compare the performance of the classifier on the training and test set and provide a reason as to why one is better than the other.
```{r}
#repeating all the steps as above, except putting our test set in instead of training 

glm.probs.test <- predict(glm.train, ovarian.dataset.test, type = "response")

glm.probs.test[1:5]

glm.pred.test <- ifelse(glm.probs.test > 0.5, "M", "B")

prediction <- glm.pred.test
expected <- ovarian.dataset.test$diagnosis

table(prediction, expected)

confusionMatrix <- table(prediction, expected)

tb <- confusionMatrix[1,1] #true benign
tm <- confusionMatrix[2,2] #true malignant 
fb <- confusionMatrix[1,2] #false benign
fm <- confusionMatrix[2,1] #false malignant 

acc <- (tb+tm)/(tb+fb+fm+tm)
sprintf("Accuracy is: %f", acc)

prec <- tb/(tb+fb)
sprintf("Precision is: %f", prec)

recall <- tb/(tb+fm)
sprintf("Recall is: %f", recall)

```



### Q3.2. Repeat the same task as Q3.1. with the top 5 PCs.

welllll need to figure out q3.1 first...


??? pseudocode would be run PCA on the (raw?) training data, then pull PC1-5 to test the accuracy of the test set 

most correct way!!!!!!: 
split data first
apply pca on the training data
use the PCs from the training set to test the accuracy of the test set! 

```{r}
ovarian.pca.train <- prcomp(ovarian.dataset.train[,c(3:32)], center = TRUE, scale. = TRUE)

ovarian.dataset.train$diagnosis <- as.factor(ovarian.dataset.train$diagnosis)

diagnosis.train <- ovarian.dataset.train$diagnosis

top5PCs <- as.data.frame(ovarian.pca.train$x[,1:5])

glm.train <- glm(diagnosis.train ~. - cell_id - diagnosis, data = top5PCs, family = binomial)
##OR SHOULD IT BE THIS:
#glm.train <- glm(diagnosis.train ~ PC1 + PC2 + PC3 + PC4 + PC5, data = ovarian.dataset.train, family = binomial)

summary(glm.train)

#summary(ovarian.pca.train)

#repeating all the steps as above, except putting our test set in instead of training 

glm.probs.pca <- predict(glm.train, top5PCs, type = "response")

glm.probs.pca[1:5]

glm.pred.pca <- ifelse(glm.probs.pca > 0.5, "M", "B")

prediction <- glm.pred.pca
expected <- ovarian.dataset.train$diagnosis

table(prediction, expected)

confusionMatrix <- table(prediction, expected)

tb <- confusionMatrix[1,1] #true benign
tm <- confusionMatrix[2,2] #true malignant 
fb <- confusionMatrix[1,2] #false benign
fm <- confusionMatrix[2,1] #false malignant 

acc <- (tb+tm)/(tb+fb+fm+tm)
sprintf("Accuracy is: %f", acc)

prec <- tb/(tb+fb)
sprintf("Precision is: %f", prec)

recall <- tb/(tb+fm)
sprintf("Recall is: %f", recall)

```


### Q3.3. Compare the results between Q3.1. and Q3.2. Do the results get better or worse? Why?
TA says obvious which data will be better! 

```{r}
# The results got BETTER when using PCA.
# This makes sense, because PCA allowed us to pick the features which describe the most variance in the set,
# thus making our classifier's performance better.

```

### Q3.4. Compare the results of the clustering and classification methods. Which one gives you better result?

??? ie comparing question 2 overall with question 3??? like comparing unsupervised vs supervised?

```{r}

```

### Q3.5. Install the library "ROCR" and load it. Then, run the following lines using your trained logistic regression model:

```{r}
pred.prob <- predict(your.model, ovarian.dataset, type="response")
predict <- prediction(pred.prob, ovarian.dataset$diagnosis, label.ordering=c("B","M"))
perform <- performance(predict,"tpr","fpr")
plot(perform,colorize=TRUE)
```
This will generate an ROC curve of the performance of the classifier for a range of thresholds. Take a look at https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5 for a summary of ROC curves.

3.5.1. Given our ROC curve, what would you say it tells us about the overlap of the two classes? 
3.5.2. What can we say about the model's separability? 
3.5.3. How does an ROC curve tell us more about the model's performance than a single sensitivity/specificity measure?



### Q3.6 . Design another classifier (using a different classification method) and repeat Q3.1-3.

part 6: do part 1,2,3 aagain but for another classification model. other options are in tutorial, eg naive base, svm, random forest 

ask the team what model type they want to use! 
```{r}

```


rdh

