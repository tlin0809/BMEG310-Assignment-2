---
title: "Assignment_2"
author: "Mona Behrouzian"
date: "10/10/2023"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#code from TA to make sure my commented code does not fall off the PDF page
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80), tidy=TRUE)
```


```{r}
setwd("C:\\Users\\monab\\Documents\\R")

ovarian.dataset <- read.delim("ovarian.data", sep=",", header = FALSE)
features <- c("perimeter", "area", "smoothness", "symmetry", "concavity", paste("protein", seq(1, 25), sep=""))
names(ovarian.dataset) <- c("cell_id", "diagnosis", features) 
# paste0(features,"_mean"), paste0(features,"_se"), paste0(features,"_worst"))

```

```{r}
#getting a sense of the data that has been loaded
#columns are: ID (not a feature), diagnosis plus the 5 variables & 25 proteins (ie 30 features)

#head(ovarian.dataset)
```

# Q1. DIMENSIONALITY REDUCTION
## Q1.1. Perform PCA on the features of the dataset. How much of the variation in the data is associated with PC1?
### 42.8% of the variation in the data is associated with PC1 
```{r}
# PCA code from tutorial 1. columns 3 to 32 only are features
# scales and centers the data, required for PCA
# Outputs the features as rows, and principle components as columns, or the axes 
ovarian.pca <- prcomp(ovarian.dataset[,c(3:32)], center = TRUE, scale. = TRUE)

# Gives the Important of components
#	Shows us the standard deviation and proportion of variance
summary(ovarian.pca)
```

## Q1.2. You want to represent 90% of the variance in the data by dimensionality reduction. How many PCs do you need to achieve this? In other words, what would be the dimensionality of the reduced feature space so that you preserve 90% of the variability in the data?
### Need 9 PCs for 90% of the variance
```{r}
# we are looking for a cumulative proportion of variance of 90%

# extract the standard deviation of the PCA
std.pca <- ovarian.pca$sdev

# variance is square of std
var.pca <- std.pca^2

# proportion of var is the variance divided by sum of all variances
prop.pca <- var.pca/sum(var.pca)

# cumulative proportion is the cumulative sum of proportion of variance 
c.sum.pca <- cumsum(prop.pca)

# use "which" function to find 1st instance when the cumsum is >=90% 
which(c.sum.pca >= 0.9)[1]

```
## Q1.3. In a 2-D plot, can you plot the observations corresponding to the first two important PCs? Note, use two different colors to represent the two classes of cells.

```{r}
#install.packages("devtools")
library(devtools)
#install.packages("remotes")
remotes::install_github("vqv/ggbiplot")
library(ggbiplot)
library(ggplot2)
library(plyr)
library(scales)
library(grid)

ggbiplot(ovarian.pca, choices = c(1,2), groups = ovarian.dataset$diagnosis)
```

## Q1.4. Can you plot the "area" and "concavity" features associated with the cells?
```{r}
#ASK TA : confirm it is okay that we used ggplot? does it look okay?

library(ggplot2)
Diagnosis <- ovarian.dataset$diagnosis
ggplot(ovarian.dataset)+
  geom_point(aes(x=Area, y=Concavity, color = Diagnosis))

```

## Q1.5. What is the difference between the two plots? Which one gives you better separation between the classes and why?
```{r}
# the first plot gives better separation.

# LATERRRRRRRRRRRRRRRRR

```

## BONUS: Q1.6 (later! i think the TA said it is just 1 line of code)

```{r}
#LATERRRRRRRRRRRR
```


# Q2. CLUSTERING
*When comparing model predictions to true labels, obtain a confusion matrix and include this result in your submission. You can obtain this by using the table() function like so: table(predictions, labels)*

## Q2.1. Apply kmeans clustering on the data and identify two clusters within your dataset. What is the concordance between the clusters that you have identified and the true labels of the cells (Benign vs Malignant).

```{r}
#using code from the webpage linked in tutorial 2 by datanovia and factoextra

#install.packages("factoextra") # for beautiful graph of clusters
library(factoextra) #load library

#must scale data first
ovarian.features <- scale(ovarian.dataset[,c(3:32)]) 

# Compute k-means with k = 2
#set.seed(123)
kmeans2 <- kmeans(ovarian.features, 2, nstart = 25)

#plotting results
fviz_cluster(kmeans2, data = ovarian.features)
             

#print(km.res) # Print the results
```
```{r}
# What is the concordance between the clusters that you have identified and the true labels of the cells (Benign vs Malignant)?

#convert the clusters into diagnosis labels 
cluster <- kmeans2$cluster
predicted.diagnosis <- factor(ifelse(cluster == 1, "M", "B")) #code from Hint
expected.diagnosis <- factor(ovarian.dataset$diagnosis)

#Install required packages
#install.packages('caret')
library(caret) #library needed to create confusion matrix

#create confusion matrix
confusion.matrix <- confusionMatrix(predicted.diagnosis, expected.diagnosis)
confusion.matrix

```

## Q2.2. Repeat the kmeans analysis 10 times and report the mean accuracy across the 10 runs. Why are the results different in each run?

```{r}
### ASK TA!!! why is it giving 92% or 100-92% each time??? is that the whole point?

ten.runs <- c(1:10)

for(i in 1:10){
  kmeans2 <- kmeans(ovarian.features, 2, nstart = 25)
  cluster <- kmeans2$cluster
  predicted.diagnosis <- factor(ifelse(cluster == 1, "B", "M")) #code from Hint
  # try mean(** == ***)
  confusion.matrix <- confusionMatrix(predicted.diagnosis, expected.diagnosis)
  accuracy <- confusion.matrix$overall['Accuracy']
  ten.runs[i] <- accuracy
  ### fix 
}
ten.runs*100

mean(ten.runs)*100
```

## Q2.3. Repeat the same analysis but with the top 5 PCs.

```{r}
#option 2 

ten.runs.pca <- c(1:10)

for(i in 1:10){
  data.kmean.pca <- kmeans(ovarian.pca$x[,1:5], 2, nstart = 25)
  cluster <- data.kmean.pca$cluster
  predicted.diagnosis <- factor(ifelse(cluster == 1, "B", "M")) #code from Hint\
  # try the thing
  confusion.matrix <- confusionMatrix(predicted.diagnosis, expected.diagnosis)
  accuracy <- confusion.matrix$overall['Accuracy']
  ten.runs.pca[i] <- accuracy
  ## fix 
}
ten.runs.pca*100

mean(ten.runs.pca)*100

```
## Q2.4. Compare the results between Q2.2. and Q2.3.
# The results get worse! Why? Perhaps it was overfitted 


### Q3. CLASSIFICATION
```{r}
#Divide your data into training and test sets using the following command:
ovarian.dataset.train <- ovarian.dataset[sample(nrow(ovarian.dataset))[1:(nrow(ovarian.dataset)/2)],]
ovarian.dataset.test <- ovarian.dataset[sample(nrow(ovarian.dataset))[(nrow(ovarian.dataset)/2):(nrow(ovarian.dataset))],]
```
"sample" randomly takes half of the data as training data, other half for testing 

### Q3.1. Design a logistic regression classifier to identify (differentiate) benign and malignant cells. Report the performance of the classification technique on the training and test sets. You can report accuracy, precision and recall. Compare the performance of the classifier on the training and test set and provide a reason as to why one is better than the other.

3.1 use logistic regression, like the tutorial 
show the table!!! must! graded!
based on the table, can calculate several things : a and p and r
but dont do it manually. use code.



We need to first convert all the M labels to be 1 and the Benign labels to be 0
```{r}
ovarian.dataset.train$diagnosis <- as.factor(ovarian.dataset.train$diagnosis)
```


```{r}
glm.training <- glm(diagnosis ~. - cell_id, data = ovarian.dataset.train, family = binomial)
summary(glm.training)
```


### Q3.3. Compare the results between Q3.1. and Q3.2. Do the results get better or worse? Why?
TA says obvious which data will be better?

make sure to use type=response 


use the link for ROC 

part 6: do part 1,2,3 aagain but for another classification model. other options are in tutorial, eg naive base, svm, random forest 





most correct way!!!!!!: 
split data first
apply pca on the training data
use the PCs from the training set to test the accuracy of the test set! 


