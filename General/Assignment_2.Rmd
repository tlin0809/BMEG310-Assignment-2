---
title: "Assignment_2"
author: "Mona Behrouzian"
date: "10/10/2023"
output: pdf_document
---

Set up code
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#code from TA to make sure my commented code does not fall off the PDF page
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80), tidy=TRUE)

setwd("C:\\Users\\monab\\Documents\\R")

ovarian.dataset <- read.delim("ovarian.data", sep=",", header = FALSE)
features <- c("perimeter", "area", "smoothness", "symmetry", "concavity", paste("protein", seq(1, 25), sep=""))
names(ovarian.dataset) <- c("cell_id", "diagnosis", features) 
# paste0(features,"_mean"), paste0(features,"_se"), paste0(features,"_worst"))

#getting a sense of the data that has been loaded
#columns are: ID (not a feature), diagnosis plus the 5 variables & 25 proteins (ie 30 features)

#head(ovarian.dataset)
```

# Q1. DIMENSIONALITY REDUCTION
## Q1.1. Perform PCA on the features of the dataset. How much of the variation in the data is associated with PC1?
*ANSWER: 42.8% of the variation in the data is associated with PC1*
```{r}
# PCA code from tutorial 1. columns 3 to 32 only are features
# scales and centers the data, required for PCA
# Outputs the features as rows, and principle components as columns, or the axes 
ovarian.pca <- prcomp(ovarian.dataset[,c(3:32)], center = TRUE, scale. = TRUE)

# Gives the Important of components
#	Shows us the standard deviation and proportion of variance
summary(ovarian.pca)
```

## Q1.2. You want to represent 90% of the variance in the data by dimensionality reduction. How many PCs do you need to achieve this? In other words, what would be the dimensionality of the reduced feature space so that you preserve 90% of the variability in the data?
*ANSWER: Need 9 PCs for 90% of the variance*
```{r}
# we are looking for a cumulative proportion of variance of 90%

# extract the standard deviation of the PCA
std.pca <- ovarian.pca$sdev

# variance is square of std
var.pca <- std.pca^2

# proportion of var is the variance divided by sum of all variances
prop.pca <- var.pca/sum(var.pca)

# cumulative proportion is the cumulative sum of proportion of variance 
c.sum.pca <- cumsum(prop.pca)

# use "which" function to find 1st instance when the cumsum is >=90% 
which(c.sum.pca >= 0.9)[1]
```
## Q1.3. In a 2-D plot, can you plot the observations corresponding to the first two important PCs? Note, use two different colors to represent the two classes of cells.
*ANSWER: See plot below*
```{r}
#install.packages("devtools") #in console
library(devtools)
#install.packages("remotes") #in console
remotes::install_github("vqv/ggbiplot")
library(ggbiplot)
library(ggplot2)
library(plyr)
library(scales)
library(grid)

ggbiplot(ovarian.pca, choices = c(1,2), groups = ovarian.dataset$diagnosis)
```

## Q1.4. Can you plot the "area" and "concavity" features associated with the cells?
*ANSWER: See plot below*
```{r}
library(ggplot2)
Diagnosis <- ovarian.dataset$diagnosis
Area <- ovarian.dataset$area
Concavity <- ovarian.dataset$concavity

ggplot(ovarian.dataset)+
  geom_point(aes(x=Area, y=Concavity, color = Diagnosis))
```

## Q1.5. What is the difference between the two plots? Which one gives you better separation between the classes and why?
*The clustered plot gives better separation compared to the second plot. This makes sense because the clustered plot used PCA to determine which two features define the most variance in the data set, whereas just ploting area vs concavity is simply selecting two features to plot at random.*
*PCA determines new axes that gives the most variance in our dataset so we are better able to cluster our observations.* 

## BONUS: Q1.6 
(later! i think the TA said it is just 1 line of code)

```{r}
#LATERRRRRRRRRRRR
```


------------------------------------------------------------------------------------------------------------

# Q2. CLUSTERING
Note: When comparing model predictions to true labels, obtain a confusion matrix and include this result in your submission. You can obtain this by using the table() function like so: table(predictions, labels)

## Q2.1 Part 1: Apply kmeans clustering on the data and identify two clusters within your dataset. 
*ANSWER: See clusters in plot below* 
```{r}
#using code from the webpage linked in tutorial 2 by datanovia and factoextra

#install.packages("factoextra") # for beautiful graph of clusters
library(factoextra) #load library

#must scale data first
ovarian.features <- scale(ovarian.dataset[,c(3:32)]) 

# Compute k-means with k = 2
#set.seed(123)
kmeans2 <- kmeans(ovarian.features, 2, nstart = 25)

#plotting results
fviz_cluster(kmeans2, data = ovarian.features)
             
# Print the results
#print(km.res)
```

PART 2: What is the concordance between the clusters that you have identified and the true labels of the cells (Benign vs Malignant).
*ANSWER: accuracy, precision, and recall listed below.*
```{r}
#First must convert the clusters into diagnosis labels 
cluster <- kmeans2$cluster
predicted.diagnosis <- factor(ifelse(cluster == 1, "M", "B")) #code from Hint
expected.diagnosis <- factor(ovarian.dataset$diagnosis)

# #Install required packages
# #install.packages('caret')
# library(caret) #library needed to create confusion matrix

#create confusion matrix using table() function
confusionMatrix <- table(predicted.diagnosis, expected.diagnosis)
confusionMatrix

tb <- confusionMatrix[1,1] #true benign
tm <- confusionMatrix[2,2] #true malignant 
fb <- confusionMatrix[1,2] #false benign
fm <- confusionMatrix[2,1] #false malignant 

acc <- (tb+tm)/(tb+fb+fm+tm)*100
sprintf("Accuracy is: %f percent", acc)

prec <- tb/(tb+fb)*100
sprintf("Precision is: %f percent", prec)

recall <- tb/(tb+fm)*100
sprintf("Recall is: %f percent", recall)
```

## Q2.2. Repeat the kmeans analysis 10 times and report the mean accuracy across the 10 runs. Why are the results different in each run?
*ANSWER: The results differ each time you run kmeans, because this algorithm randomly picks initial cluster centers, and then iterates to minimize the within-cluster sum of squares error.* 
*In other words, the mean accuracy across the 10 runs differs every time you run k-means.*
```{r}
ten.runs <- c(1:10) #initialize a variable

for(i in 1:10){
  kmeans2 <- kmeans(ovarian.features, 2, nstart = 25) #2 for 2 clusters, >=25 recommended for nStart
  cluster <- kmeans2$cluster
  predicted.diagnosis <- factor(ifelse(cluster == 1, "B", "M")) #code from Hint

  #create confusion matrix using table() function
  confusionMatrix <- table(predicted.diagnosis, expected.diagnosis)

  tb <- confusionMatrix[1,1] #true benign
  tm <- confusionMatrix[2,2] #true malignant 
  fb <- confusionMatrix[1,2] #false benign
  fm <- confusionMatrix[2,1] #false malignant 
  
  accuracy <- (tb+tm)/(tb+fb+fm+tm)

  ten.runs[i] <- accuracy
  #TA:  fix code to subtract from 100%
}
ten.runs*100

sprintf("Mean Accuracy across the 10 runs is: %f percent", mean(ten.runs)*100)
```

## Q2.3. Repeat the same analysis but with the top 5 PCs.
```{r}
ten.runs.pca <- c(1:10)

for(i in 1:10){
  data.kmean.pca <- kmeans(ovarian.pca$x[,1:5], 2, nstart = 25)
  cluster <- data.kmean.pca$cluster
  predicted.diagnosis <- factor(ifelse(cluster == 1, "B", "M")) #code from Hint

  #create confusion matrix using table() function
  confusionMatrix <- table(predicted.diagnosis, expected.diagnosis)

  tb <- confusionMatrix[1,1] #true benign
  tm <- confusionMatrix[2,2] #true malignant 
  fb <- confusionMatrix[1,2] #false benign
  fm <- confusionMatrix[2,1] #false malignant 
  
  accuracy <- (tb+tm)/(tb+fb+fm+tm)
  ten.runs.pca[i] <- accuracy
  #TA:  fix code to subtract from 100%
}
ten.runs.pca*100

sprintf("Mean Accuracy across the 10 runs (but using top 5 PCs) is: %f percent", mean(ten.runs.pca)*100)
```

## Q2.4. Compare the results between Q2.2. and Q2.3.
*ANSWER: The mean accuracy across the 10 runs was about 50% for both using k-means for clustering the original data set AND for clustering the top 5 principle components*
*These similar results suggest that the top 5 PCs capture enough variance as compared to including all the features.*
*In other words, the top 5 PCs do a good job at representing the variance of the whole data set.*


# Q3. CLASSIFICATION

Set up:
```{r}
#Divide your data into training and test sets using the following command:
ovarian.dataset.train <- ovarian.dataset[sample(nrow(ovarian.dataset))[1:(nrow(ovarian.dataset)/2)],]
ovarian.dataset.test <- ovarian.dataset[sample(nrow(ovarian.dataset))[(nrow(ovarian.dataset)/2):(nrow(ovarian.dataset))],]
#"sample" randomly takes half of the data as training data, other half for testing 

# install.packages('ISLR') #in console
#require(ISLR)

#Need to see how our output is dependent on our input. To do that, we can use correlation, 
# ie how much the variation in our output is realted to our input. 
#Or, if i change the input, how much that affects the output.

# # install this for correlation install.packages('corrplot')
# library(corrplot)
# 
# correlations <- cor(ovarian.dataset.train[, 3:32])
# #correlations
# 
# corrplot(correlations, method = "circle")

#Now, we can see the correlation plot above. We used circles. Rows and cols are features. 
#We are measuring correlation between alllll the pairs of features. 
#Larger the circle, higher the correlation. Dark blue means huge correlation with the data
```

### Q3.1. Design a logistic regression classifier to identify (differentiate) benign and malignant cells. 
Part 1: Report the performance of the classification technique on the training and test sets. You can report accuracy, precision and recall. 
```{r}
#We need to first convert all the char variables to be numbers (help from TA!)
#note we don't know if M becomes 1 or 0, vice versa for B. We just trust the function.
ovarian.dataset.train$diagnosis <- as.factor(ovarian.dataset.train$diagnosis)

expected <- ovarian.dataset.train$diagnosis

#trying to scale the data first, but gives errors!
# ovarian.dataset.train <- as.data.frame(scale(ovarian.dataset.train[3:32]))
# ovarian.dataset.test <- as.data.frame(scale(ovarian.dataset.test[3:32]))

#Creating the logistic regression model.
#using the binomial argument to make it clear that the intent is to git a logistic regression model
# the "." means we are taking all the features, but not including cell_id nor diagnosis
log.reg.model <- glm(expected ~. - cell_id - diagnosis, data = ovarian.dataset.train, family = binomial)
summary(log.reg.model)

#note that this gives the error:
# "glm.fit: algorithm did not converge glm.fit: fitted probabilities numerically 0 or 1 occurred"
#but, that just means that the function was able to perfectly separate the variable into 0's and 1's.
#this is what we wanted!
```
Note that ALLLLL the p-values are so HIGH! Thus, none of the coefficients are significant here. 
To improve the model, you would drop the features based on which ones have high/bad p value 

We now have the model trained, but need to get the prediction from it.
Need to extract the probabilities from the model, using the prediction function!
```{r}
# input our model, then features, then the type. If you don't use the type, it will NOT GIVE YOU THE PROBABILITY!!!!
probabilities <- predict(log.reg.model, ovarian.dataset.train, type = "response")
#the function knows which features to use in the dataset since we set it up in glm

probabilities[1:5]
```
These probabilities are very far from 0.5, ie they are extremely close to 0 or 1, which is good! Our model is classifying strongly. 

So we have the probabilities, but how to convert them to labels? 
Consider the typical s-shape logistic reg curve. If the prob is >0.5 it is yes, <0.5 it is 0. Because it is binary. 
We simply need to use ifelse to do this. It will convert each of the probabilities to the B or M label.
```{r}
prediction <- ifelse(probabilities > 0.5, "M", "B")`

expected <- ovarian.dataset.train$diagnosis

confusionMatrix <- table(prediction, expected)
confusionMatrix
```
There are no false positives/malignant or false negatives/benign. This makes sense because we are testing against our training set, so of course our model seems to classify "perfectly".

Using this table, we can manually calculate many statistics such as accuracy, precision, and recall of the prediction.
```{r}
tb <- confusionMatrix[1,1] #true benign
tm <- confusionMatrix[2,2] #true malignant 
fb <- confusionMatrix[1,2] #false benign
fm <- confusionMatrix[2,1] #false malignant 

acc <- (tb+tm)/(tb+fb+fm+tm)*100
sprintf("Accuracy is: %f", acc)

prec <- tb/(tb+fb)*100
sprintf("Precision is: %f", prec)

recall <- tb/(tb+fm)*100
sprintf("Recall is: %f", recall)
```
*ANSWER: Thus, the accuracy, precision, and recall of our model all seems to be 100% - which makes sense because we are testing our model with the training set we used to build the model in the first place.*

PART 2: Compare the performance of the classifier on the training and test set and provide a reason as to why one is better than the other.
```{r}
#repeating all the steps as above, except putting our *test* set in instead of training 

ovarian.dataset.test$diagnosis <- as.factor(ovarian.dataset.test$diagnosis)
expected.test <- ovarian.dataset.test$diagnosis

#log.reg.model.test <- glm(expected.test ~. - cell_id - diagnosis, data = ovarian.dataset.test, family = binomial)
#summary(log.reg.model.test)

probabilities.test <- predict(log.reg.model, ovarian.dataset.test, type = "response")

probabilities.test[1:5]

prediction.test <- ifelse(probabilities.test > 0.5, "M", "B")

confusionMatrix.test <- table(prediction.test, expected.test)
confusionMatrix.test

tb <- confusionMatrix.test[1,1] #true benign
tm <- confusionMatrix.test[2,2] #true malignant 
fb <- confusionMatrix.test[1,2] #false benign
fm <- confusionMatrix.test[2,1] #false malignant 

acc <- (tb+tm)/(tb+fb+fm+tm)*100
sprintf("(3.1) Accuracy is: %f", acc)

prec <- tb/(tb+fb)*100
sprintf("(3.1) Precision is: %f", prec)

recall <- tb/(tb+fm)*100
sprintf("(3.1) Recall is: %f", recall)

```
*ANSWER: the accuracy, precision, and recall from running our test set through our model are very high - but not all 100% like we say before.*
*This makes sense, because it's the first time the model is seeing this dataset, so it's expected to make some mistakes in classifying.*

### Q3.2. Repeat the same task as Q3.1. with the top 5 PCs.

Tutorial: most correct way!!!!!!: 
split data first
apply pca on the training data
use the PCs from the training set to test the accuracy of the test set! 

```{r}
#Apply PCA on raw training set
train.pca <- prcomp(ovarian.dataset.train[,c(3:32)], center = TRUE, scale. = TRUE)
summary(train.pca)

top5PCs <- train.pca$x[,1:5] #pull out the top 5 principle components 

#Converting test set to compare with.
test.pca <- predict(train.pca, ovarian.dataset.test[,c(3:32)], type = "response")

ovarian.dataset.train$diagnosis <- as.factor(ovarian.dataset.train$diagnosis)

top5PCs_frame <- data.frame(top5PCs, diagnosis = ovarian.dataset.train$diagnosis)
test.pca_frame <- data.frame(test.pca, diagnosis = ovarian.dataset.test$diagnosis)

log.reg.model.pca <- glm(diagnosis ~., data = top5PCs_frame, family = binomial)
summary(log.reg.model.pca)

probabilities.pca <- predict(log.reg.model.pca, test.pca_frame, type = "response")
probabilities.pca[1:5]

prediction <- ifelse(probabilities.pca > 0.5, "M", "B")

confusionMatrix <- table(prediction, ovarian.dataset.test$diagnosis)
confusionMatrix

tb <- confusionMatrix[1,1] #true benign
tm <- confusionMatrix[2,2] #true malignant 
fb <- confusionMatrix[1,2] #false benign
fm <- confusionMatrix[2,1] #false malignant 

acc <- (tb+tm)/(tb+fb+fm+tm)*100
sprintf("Accuracy using PCA is: %f", acc)

prec <- tb/(tb+fb)*100
sprintf("Precision using PCA is: %f", prec)

recall <- tb/(tb+fm)*100
sprintf("Recall using PCA is: %f", recall)

```


### Q3.3. Compare the results between Q3.1. and Q3.2. Do the results get better or worse? Why?
*ANSWER: The results got BETTER when using PCA. This makes sense, because PCA allowed us to pick the features which describe the most variance in the set, thus making our classifier's performance better.*

### Q3.4. Compare the results of the clustering (Q2) and classification (Q3) methods. Which one gives you better result?
TA: you should compare non-pca with non-pca and pca with pca. 
*ANSWER: For both clustering and classification, we got a higher accuracy when using PCA*
*For clustering, without PCA we got roughly 50% accuracy, but with PCA, we got similar but typically higher accuracy, roughly 60%.*
*For classification, without PCA we got 97% accuracy, but with PCA, we got 98%.*
*Overall, PCA led us to have similar accuracies, but always HIGHER accuracies.*

*This makes sense as PCA allows us to extract the features that describe the most amount of variance in our dataset.*
*Thus, we can expect to have more accurate models when using PCA.*

*talk about the centroid and stuff*

compare accuracy (kmeans and log.reg.model.test) vs accuracy (kmean&pca and log.reg.model&pca)

### Q3.5. Install the library "ROCR" and load it. Then, run the following lines using your trained logistic regression model:
```{r}
#install.packages("ROCR") #in console
library(ROCR)
pred.prob <- predict(log.reg.model, ovarian.dataset, type="response")
predict <- prediction(pred.prob, ovarian.dataset$diagnosis, label.ordering=c("B","M"))
perform <- performance(predict,"tpr","fpr")
plot(perform,colorize=TRUE)
```
This will generate an ROC curve of the performance of the classifier for a range of thresholds. Take a look at https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5 for a summary of ROC curves.

3.5.1. Given our ROC curve, what would you say it tells us about the overlap of the two classes? 
*ANSWER: based on the shape, there is very little overlap*

3.5.2. What can we say about the model's separability? 
*ANSWER: high separability.*

3.5.3. How does an ROC curve tell us more about the model's performance than a single sensitivity/specificity measure?
*ANSWER: sometimes accuracy alone isngt giving us the info we are looking for. ROC gives robustness *
*depending on our application, roc tells us what threshold to use *
*useful for imbalanced problem!!! *


### Q3.6 . Design another classifier (using a different classification method) and repeat Q3.1-3.

part 6: do part 1,2,3 again but for another classification model. other options are in tutorial, eg naive base, svm, random forest 

```{r}
 
```
